{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cd6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najo/.conda/envs/lerobot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-01 21:03:45.525304: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-01 21:03:45.569668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-01 21:03:45.569707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-01 21:03:45.571043: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-01 21:03:45.578440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-01 21:03:46.459073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Processor & Model from 'declare-lab/nora'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing Qwen VLM parameters...\n",
      "  ✓ Vision encoder frozen\n",
      "  ✓ Language model frozen\n",
      "  ✓ LM head (action decoder) trainable\n",
      "  Trainable parameters: 314,804,224 / 3,758,262,272 (8.38%)\n",
      "Loading FAST tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: action_dim, vocab_size, time_horizon, scale, min_token. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating dummy data...\n",
      "Processing example...\n",
      "\n",
      "[Processed Messages Structure]:\n",
      "User: Pick up the red apple on the table.\n",
      "Assistant (Action Tokens): <robot_action_444><robot_action_277><robot_action_257><robot_action_308>\n",
      "\n",
      "Running Forward Pass (Test)...\n",
      "\n",
      "[Generation Result]:\n",
      "['system\\nYou are a helpful assistant.\\nuser\\nPick up the red apple on the table.\\nassistant\\n\\nassistant\\n']\n",
      "\n",
      "Test Complete! Code is valid.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from accelerate import Accelerator\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# --- 0. Mock Utils (실제 환경에 토크나이저가 없을 경우를 대비한 임시 클래스) ---\n",
    "class MockFastTokenizer:\n",
    "    \"\"\"physical-intelligence/fast 토크나이저가 없을 때 테스트용\"\"\"\n",
    "    def __call__(self, action):\n",
    "        # 액션 차원만큼 임의의 토큰 ID 리스트 반환 (예: 0~255 사이)\n",
    "        return [(np.array(action) * 10).astype(int).flatten().tolist()]\n",
    "\n",
    "# --- 1. Model & Processor Loading ---\n",
    "def load_model_and_processor(accelerator: Accelerator):\n",
    "    \"\"\"\n",
    "    Load NORA model and processors.\n",
    "    \"\"\"\n",
    "    accelerator.print(f\"Loading Processor & Model from 'declare-lab/nora'...\")\n",
    "    \n",
    "    # 1. Load Processor\n",
    "    try:\n",
    "        processor = AutoProcessor.from_pretrained('declare-lab/nora', trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Warning: Failed to load specific processor, using default Qwen2.5-VL. Error: {e}\")\n",
    "        processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "    processor.tokenizer.padding_side = 'left'\n",
    "\n",
    "    # 2. Load Model\n",
    "    # GLIBC 호환성 문제시 attn_implementation=\"eager\" 또는 \"sdpa\" 사용\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        'declare-lab/nora',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\", \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # 3. Freeze Logic\n",
    "    accelerator.print(\"Freezing Qwen VLM parameters...\")\n",
    "\n",
    "    # Freeze vision encoder\n",
    "    if hasattr(model, 'visual'):\n",
    "        for param in model.visual.parameters():\n",
    "            param.requires_grad = False\n",
    "        accelerator.print(\"  ✓ Vision encoder frozen\")\n",
    "\n",
    "    # Freeze language model backbone\n",
    "    if hasattr(model, 'model'):\n",
    "        for param in model.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        accelerator.print(\"  ✓ Language model frozen\")\n",
    "\n",
    "    # Keep lm_head trainable\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        for param in model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        accelerator.print(\"  ✓ LM head (action decoder) trainable\")\n",
    "\n",
    "    # Count parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    accelerator.print(f\"  Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "    # 4. Load FAST tokenizer\n",
    "    try:\n",
    "        accelerator.print(\"Loading FAST tokenizer...\")\n",
    "        fast_tokenizer = AutoProcessor.from_pretrained(\n",
    "            \"physical-intelligence/fast\", trust_remote_code=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Warning: Could not load 'physical-intelligence/fast'. Using MockTokenizer for testing. Error: {e}\")\n",
    "        fast_tokenizer = MockFastTokenizer()\n",
    "\n",
    "    return model, processor, fast_tokenizer\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "def map_fast_token_to_vlm_action(tokens: List[int]) -> str:\n",
    "    \"\"\"\n",
    "    Maps FAST action tokens to VLM action format string.\n",
    "    \"\"\"\n",
    "    return ''.join([f\"<robot_action_{token}>\" for token in tokens])\n",
    "\n",
    "def process_example_for_nora(example: Dict[str, Any], processor: AutoProcessor, fast_tokenizer: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a single example for NORA training format.\n",
    "    \"\"\"\n",
    "    # 1. Get Action\n",
    "    action = example['action']\n",
    "    if isinstance(action, torch.Tensor):\n",
    "        action = action.numpy()\n",
    "    \n",
    "    if len(action.shape) == 1:\n",
    "        action = action[np.newaxis, :]  # (1, action_dim)\n",
    "\n",
    "    # 2. Tokenize Action\n",
    "    # fast_tokenizer returns input_ids or raw tokens depending on implementation.\n",
    "    # Here we assume it returns a list of token IDs per sample.\n",
    "    fast_tokens_batch = fast_tokenizer(action)\n",
    "    \n",
    "    # Handle Mock vs Real return types\n",
    "    if isinstance(fast_tokens_batch, dict) and 'input_ids' in fast_tokens_batch:\n",
    "        fast_tokens = fast_tokens_batch['input_ids'][0]\n",
    "    elif isinstance(fast_tokens_batch, list):\n",
    "        fast_tokens = fast_tokens_batch[0]\n",
    "    else:\n",
    "        fast_tokens = fast_tokens_batch # Fallback\n",
    "\n",
    "    vlm_action_str = map_fast_token_to_vlm_action(fast_tokens)\n",
    "\n",
    "    # 3. Process Image\n",
    "    # example['observation.images.camera1'] is assumed (C, H, W) Tensor\n",
    "    pixel_values = example['observation.images.camera1'] \n",
    "    \n",
    "    # (C, H, W) -> (H, W, C) & Scale\n",
    "    img_np = pixel_values.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(img_np)\n",
    "\n",
    "    # 4. Construct Message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": pil_image},\n",
    "                {\"type\": \"text\", \"text\": example['task']},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": vlm_action_str},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "# --- 3. Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Accelerator Init\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # 2. Load System\n",
    "    model, processor, fast_tokenizer = load_model_and_processor(accelerator)\n",
    "    model.eval() # Test mode\n",
    "\n",
    "    # 3. Create Dummy Data (Mocking HDF5 Dataset output)\n",
    "    accelerator.print(\"\\nGenerating dummy data...\")\n",
    "    dummy_example = {\n",
    "        'observation.images.camera1': torch.rand(3, 224, 224), # Random Image (C, H, W)\n",
    "        'action': torch.tensor([0.1, 0.2, 0.3, 0.0, 0.0, 1.0]), # 6-DOF Action\n",
    "        'task': \"Pick up the red apple on the table.\"\n",
    "    }\n",
    "\n",
    "    # 4. Process Data\n",
    "    accelerator.print(\"Processing example...\")\n",
    "    messages = process_example_for_nora(dummy_example, processor, fast_tokenizer)\n",
    "    \n",
    "    # Debug: Print structure\n",
    "    accelerator.print(\"\\n[Processed Messages Structure]:\")\n",
    "    print(f\"User: {messages[0]['content'][1]['text']}\")\n",
    "    print(f\"Assistant (Action Tokens): {messages[1]['content'][0]['text']}\")\n",
    "\n",
    "    # 5. Prepare Inputs for Model (Creating Tensors)\n",
    "    text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    image_inputs, video_inputs = [], []\n",
    "    for msg in messages:\n",
    "        for content in msg[\"content\"]:\n",
    "            if content[\"type\"] == \"image\":\n",
    "                image_inputs.append(content[\"image\"])\n",
    "            # 비디오 처리 로직이 있다면 여기에 추가\n",
    "    \n",
    "    # [수정됨] 비디오 리스트가 비어있으면 None을 전달해야 에러가 안 납니다.\n",
    "    final_video_inputs = video_inputs if len(video_inputs) > 0 else None\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text_prompt],\n",
    "        images=image_inputs,\n",
    "        videos=final_video_inputs,  # <--- 수정된 부분 ([] 대신 None 전달)\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs = inputs.to(model.device)\n",
    "    # 6. Inference Test (Forward Pass)\n",
    "    accelerator.print(\"\\nRunning Forward Pass (Test)...\")\n",
    "    with torch.no_grad():\n",
    "        # labels logic is needed for training, here we just check generation or forward shape\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "        \n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "    accelerator.print(\"\\n[Generation Result]:\")\n",
    "    accelerator.print(generated_text)\n",
    "    \n",
    "    accelerator.print(\"\\nTest Complete! Code is valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da90ad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521af82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: flash-attn 2.6.1\n",
      "Uninstalling flash-attn-2.6.1:\n",
      "  Successfully uninstalled flash-attn-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4717c3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn==2.6.1+cu123torch2.4cxx11abifalse\n",
      "  Downloading https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl (198.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.5/198.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:21\u001b[0m[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (2.4.0)\n",
      "Requirement already satisfied: einops in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (0.8.1)\n",
      "Requirement already satisfied: filelock in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/najo/.conda/envs/lerobot/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.6.1+cu123torch2.4cxx11abifalse) (1.3.0)\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.1/flash_attn-2.6.1+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a61244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
